{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE9010: Introduction to Data Science\n",
    "## Semester 2 2017/18\n",
    "## Xavier Bresson\n",
    "<hr>\n",
    "\n",
    "## Tutorial 3: Linear supervised regression\n",
    "## Objectives\n",
    "### $\\bullet$ Code a linear regression model \n",
    "### $\\bullet$ Implement gradient descent \n",
    "### $\\bullet$ Explore results\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# math library\n",
    "import numpy as np\n",
    "\n",
    "# visualization library\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 3d visualization\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# computational time\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Load dataset\n",
    "<hr>\n",
    "The data feature, $x$, is unique and represents the population size of various cities. <br>\n",
    "The data label/target, $y$, to predict is the profit.\n",
    "\n",
    "What is the number $n$ of training data?<br>\n",
    "Hint: You may use numpy function `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data with numpy\n",
    "data = np.loadtxt('data/profit_population.txt', delimiter=',')\n",
    "\n",
    "# number of training data\n",
    "n =  #YOUR CODE HERE\n",
    "print('Number of training data=',n)\n",
    "\n",
    "# print\n",
    "print(data[:10,:])\n",
    "print(data.shape)\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the dataset distribution\n",
    "<hr>\n",
    "\n",
    "Plot the training data points.<br>\n",
    "Hint: You may use matplotlib function `scatter(x,y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:,0]\n",
    "y_train = data[:,1]\n",
    "\n",
    "plt.figure(1)\n",
    "plt. #YOUR CODE HERE\n",
    "plt.title('Training data')\n",
    "plt.xlabel('Population size (x 10$)')\n",
    "plt.ylabel('Profit (x 10k$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the linear prediction function \n",
    "<hr>\n",
    "$$\n",
    "f_w(x) = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "There are 2 possible implementations:\n",
    "\n",
    "1. Unvectorized implementation (with *for* loops): \n",
    "$$\n",
    "f_w(x_i) = w_0 + w_1 x_i\n",
    "$$\n",
    "<br>\n",
    "\n",
    "1. Vectorized implementation:\n",
    "$$\n",
    "f_w(x) = X w \n",
    "$$\n",
    "with \n",
    "<br>\n",
    "$$\n",
    "X = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "1 & x_1 \\\\ \n",
    "1 & x_2 \\\\ \n",
    "\\vdots\\\\\n",
    "1 & x_n\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\textrm{ and }\n",
    "\\quad\n",
    "w = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "w_0 \\\\ \n",
    "w_1 \n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\Rightarrow \n",
    "\\quad\n",
    "f_w(x) = X w  =\n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "w_0 + w_1 x_1 \\\\ \n",
    "w_0 + w_1 x_2 \\\\ \n",
    "\\vdots\\\\\n",
    "w_0 + w_1 x_n\n",
    "\\end{array} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Implement the vectorized version of the linear predictive function. <br>\n",
    "\n",
    "Check your code correctness: The first 5 values of $f_w(x)$ are [-8.35,-7.53,-11.72,-9.60, -8.00] with $w_0=0.2, w_1=-1.4$. <br>\n",
    "\n",
    "Hint: Respect the sizes of $X$ and $w$ when carrying out linear algebra multiplications. You may use numpy function `dot` for matrix multiplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct data matrix\n",
    "X = np.ones([n,2]) \n",
    "X[:,1] = x_train\n",
    "print(X.shape)\n",
    "print(X[:5,:])\n",
    "\n",
    "\n",
    "# parameters vector\n",
    "w = np.array([0.2,-1.4])[:,None] # [:,None] adds a singleton dimension\n",
    "print(w.shape)\n",
    "#print(w)\n",
    "\n",
    "\n",
    "# predictive function definition\n",
    "def f_pred(X,w): \n",
    "    f =  #YOUR CODE HERE\n",
    "    return f \n",
    "\n",
    "\n",
    "# Test predicitive function \n",
    "y_pred = f_pred(X,w)\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the linear regression loss \n",
    "<hr>\n",
    "$$\n",
    "L(w)=\\frac{1}{n} \\sum_{i=1}^n \\ \\Big( f_w(x_i) – y_i \\Big)^2\n",
    "$$\n",
    "\n",
    "There are again 2 possible implementations:\n",
    "1. Unvectorized implementation. \n",
    "\n",
    "1. Vectorized implementation:\n",
    "$$\n",
    "L(w)=\\frac{1}{n} (Xw-y)^T(Xw-y)\n",
    "$$\n",
    "with \n",
    "<br>\n",
    "$$\n",
    "Xw=\n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "w_0 + w_1 x_1 \\\\ \n",
    "w_0 + w_1 x_2 \\\\ \n",
    "\\vdots\\\\\n",
    "w_0 + w_1 x_n\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\textrm{ and }\n",
    "\\quad\n",
    "y = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Implement the vectorized version of the linear regression loss function. <br>\n",
    "\n",
    "Check your code correctness: The loss values is $399.75$ for $w_0=0.2, w_1=-1.4$. <br>\n",
    "\n",
    "Hint: Respect the sizes of $X$, $w$ and $y$ when carrying out linear algebra multiplications. You may use numpy function transpose `.T`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function definition\n",
    "def loss_mse(y_pred,y): \n",
    "    n = len(y)\n",
    "    loss =  #YOUR CODE HERE\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Test loss function \n",
    "y = y_train[:,None] # label \n",
    "#print(y.shape)\n",
    "y_pred = f_pred(X,w) # prediction\n",
    "loss = loss_mse(y_pred,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the gradient of the linear regression loss \n",
    "<hr>\n",
    "\n",
    "$\\bullet$ Unvectorized implementation:\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial w_0} L(w)&=&\\frac{2}{n} \\sum_{i=1}^n \\ ( w_0+w_1x_i - y_i )\\\\\n",
    "\\frac{\\partial}{\\partial w_1} L(w)&=&\\frac{2}{n} \\sum_{i=1}^n \\ ( w_0+w_1x_i - y_i )x_i\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\bullet$ Vectorized implementation: Given the loss\n",
    "$$\n",
    "L(w)=\\frac{1}{n} (Xw-y)^T(Xw-y)\n",
    "$$\n",
    "The gradient is given by  \n",
    "$$\n",
    "\\frac{\\partial}{\\partial w} L(w) = \\frac{2}{n} X^T(Xw-y)\n",
    "$$\n",
    "\n",
    "\n",
    "Implement the vectorized version of the gradient of the linear regression loss function. <br>\n",
    "\n",
    "Check your code correctness: The gradient value is [-34.12,-355.32] for $w_0=0.2, w_1=-1.4$. <br>\n",
    "\n",
    "Hint: Respect the sizes of $X$, $w$ and $y$ when carrying out linear algebra multiplications. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gradient function definition\n",
    "def grad_loss(y_pred,y,X):\n",
    "    n = len(y)\n",
    "    grad =  #YOUR CODE HERE\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Test grad function \n",
    "y_pred = f_pred(X,w)\n",
    "grad = grad_loss(y_pred,y,X)\n",
    "print(grad)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement the gradient descent algorithm \n",
    "<hr>\n",
    "\n",
    "$\\bullet$ Unvectorized implementation:\n",
    "\\begin{eqnarray}\n",
    "w_0^{k+1} &= w_0^{k}& - \\tau \\frac{2}{n} \\sum_{i=1}^n \\ ( w_0^{k}+w_1^{k}x_i - y_i ) \\\\\n",
    "w_1^{k+1} &= w_1^{k}& - \\tau \\frac{2}{n} \\sum_{i=1}^n \\ ( w_0^{k}+w_1^{k}x_i - y_i ) x_i\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "$\\bullet$ Vectorized implementation: \n",
    "$$\n",
    "w^{k+1} = w^{k} - \\tau  \\frac{2}{n} X^T(Xw^{k}-y)\n",
    "$$\n",
    "\n",
    "**6.1** Implement the vectorized version of the gradient descent function. <br>\n",
    "Check your code correctness: The $w^{k}$ value after $20$ iterations is [0.116,0.789] for initial values $w_0^{k=0}=0.2, w_1^{k=0}=-1.4$ and the loss $L$ is 11.90.<br>\n",
    "\n",
    "\n",
    "**6.2** Plot the loss values $L(w^k)$ w.r.t. iteration $k$ the number of iterations.<br>\n",
    "\n",
    "Hint: You may use a table to store the values of $L(w^k)$ at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent function definition\n",
    "def grad_desc(X, y , w_init=np.array([0,0])[:,None] ,tau=0.01, max_iter=500):\n",
    "\n",
    "    L_iters = np.zeros([max_iter]) # record the loss values\n",
    "    w_iters = np.zeros([max_iter,2]) # record the loss values\n",
    "    w = w_init # initialization\n",
    "    for i in range(max_iter): # loop over the iterations\n",
    "        y_pred = f_pred(X,w) # linear predicition function #YOUR CODE HERE\n",
    "        grad_f = grad_loss(y_pred,y,X) # gradient of the loss #YOUR CODE HERE\n",
    "        w = w - tau* grad_f # update rule of gradient descent #YOUR CODE HERE\n",
    "        L_iters[i] = loss_mse(y_pred,y) # save the current loss value \n",
    "        w_iters[i,:] = w[0],w[1] # save the current w value \n",
    "        \n",
    "    return w, L_iters, w_iters\n",
    "\n",
    "\n",
    "# run gradient descent algorithm \n",
    "start = time.time()\n",
    "w_init = np.array([0.2,-1.4])[:,None]\n",
    "tau = 0.01\n",
    "max_iter = 20\n",
    "w, L_iters, w_iters = grad_desc(X,y,w_init,tau,max_iter)\n",
    "print('Time=',time.time() - start)\n",
    "print(L_iters[-1])\n",
    "print(w)\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(2)\n",
    "plt.plot(np.array(range(max_iter)), L_iters)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot the linear prediction function\n",
    "<hr>\n",
    "$$\n",
    "f_w(x) = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "Hint: You may use numpy function `linspace`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression model\n",
    "x_pred =  #YOUR CODE HERE\n",
    "y_pred =  #YOUR CODE HERE\n",
    "\n",
    "# plot\n",
    "plt.figure(3)\n",
    "plt.scatter(x_train, y_train, s=30, c='r', marker='x', linewidths=1)\n",
    "plt.plot(x_pred, y_pred,label='gradient descent optimization'.format(i=1))\n",
    "plt.legend(loc='best')\n",
    "plt.title('Training data')\n",
    "plt.xlabel('Population size (x 10k)')\n",
    "plt.ylabel('Profit $(x 10k)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison with Scikit-learn linear regression algorithm\n",
    "<hr>\n",
    "**8.1** What is the loss value of the Scikit-learn solution? <br>\n",
    "Compare with the loss value given by gradient descent?<br>\n",
    "What do we need to do to get a better loss (and solution) with gradient descent? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run linear regression with scikit-learn\n",
    "start = time.time()\n",
    "lin_reg_sklearn = LinearRegression()\n",
    "lin_reg_sklearn.fit( ) # learn the model parameters #YOUR CODE HERE\n",
    "print('Time=',time.time() - start)\n",
    "\n",
    "\n",
    "# compute loss value\n",
    "w_sklearn = np.zeros([2,1])\n",
    "w_sklearn[0,0] = lin_reg_sklearn.intercept_\n",
    "w_sklearn[1,0] = lin_reg_sklearn.coef_\n",
    "print(w_sklearn)\n",
    "loss_sklearn = loss_mse(f_pred(X,w_sklearn),y_train[:,None])\n",
    "print('loss sklearn=',loss_sklearn)\n",
    "print('loss gradient descent=',L_iters[-1]) \n",
    "\n",
    "\n",
    "# plot\n",
    "y_pred_sklearn = w_sklearn[0] + w_sklearn[1]* x_pred\n",
    "plt.figure(3)\n",
    "plt.scatter(x_train, y_train, s=30, c='r', marker='x', linewidths=1)\n",
    "plt.plot(x_pred, y_pred,label='gradient descent optimization'.format(i=1))\n",
    "plt.plot(x_pred, y_pred_sklearn,label='Scikit-learn optimization'.format(i=2))\n",
    "plt.legend(loc='best')\n",
    "plt.title('Training data')\n",
    "plt.xlabel('Population size (x 10k)')\n",
    "plt.ylabel('Profit $(x 10k)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2** What do we need to do to get a better loss (and solution) with gradient descent?<br>\n",
    "We need more gradient descent iterations.<br>\n",
    "Run gradient descent with 1,000 iterations. Check the loss value.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent algorithm \n",
    "w_init = np.array([0.2,-1.4])[:,None]\n",
    "tau = 0.01\n",
    "max_iter = 1000\n",
    "w, L_iters, w_iters = grad_desc(X,y,w_init,tau,max_iter)\n",
    "print(L_iters[-1])\n",
    "print(w)\n",
    "\n",
    "\n",
    "# plot\n",
    "y_pred = w[0] + w[1]* x_pred\n",
    "plt.figure(4)\n",
    "plt.scatter(x_train, y_train, s=30, c='r', marker='x', linewidths=1)\n",
    "plt.plot(x_pred, y_pred,label='gradient descent optimization'.format(i=1))\n",
    "plt.plot(x_pred, y_pred_sklearn,label='Scikit-learn optimization'.format(i=2))\n",
    "plt.legend(loc='best')\n",
    "plt.title('Training data')\n",
    "plt.xlabel('Population size (x 10k)')\n",
    "plt.ylabel('Profit $(x 10k)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Predict profit for a city with population of 45,000?\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict profit for a city with population of 45000\n",
    "print('Profit would be', ) #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. [Bonus] Plot the loss surface, the contours of the loss and the gradient descent steps\n",
    "<hr>\n",
    "\n",
    "Hint: Use function *plot_gradient_descent(X,y,w_init,tau,max_iter)*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot gradient descent \n",
    "def plot_gradient_descent(X,y,w_init,tau,max_iter):\n",
    "    \n",
    "    def f_pred(X,w):\n",
    "        f = X.dot(w) \n",
    "        return f\n",
    "    \n",
    "    def loss_mse(y_pred,y):\n",
    "        n = len(y)\n",
    "        loss = 1/n* (y_pred - y).T.dot(y_pred - y)\n",
    "        return loss\n",
    "    \n",
    "    def grad_desc(X, y , w_init=np.array([0,0])[:,None] ,tau=0.01, max_iter=500):\n",
    "\n",
    "        L_iters = np.zeros([max_iter]) # record the loss values\n",
    "        w_iters = np.zeros([max_iter,2]) # record the loss values\n",
    "        w = w_init # initialization\n",
    "        for i in range(max_iter): # loop over the iterations\n",
    "            y_pred = f_pred(X,w) # linear predicition function\n",
    "            grad_f = grad_loss(y_pred,y,X) # gradient of the loss\n",
    "            w = w - tau* grad_f # update rule of gradient descent\n",
    "            L_iters[i] = loss_mse(y_pred,y) # save the current loss value\n",
    "            w_iters[i,:] = w[0],w[1] # save the current w value\n",
    "\n",
    "        return w, L_iters, w_iters\n",
    "\n",
    "    # run gradient descent\n",
    "    w, L_iters, w_iters = grad_desc(X,y,w_init,tau,max_iter)\n",
    "    \n",
    "    # Create grid coordinates for plotting a range of L(w0,w1)-values\n",
    "    B0 = np.linspace(-10, 10, 50)\n",
    "    B1 = np.linspace(-1, 4, 50)\n",
    "    xx, yy = np.meshgrid(B0, B1, indexing='xy')\n",
    "    Z = np.zeros((B0.size,B1.size))  \n",
    "\n",
    "    # Calculate loss values based on L(w0,w1)-values\n",
    "    for (i,j),v in np.ndenumerate(Z):\n",
    "        Z[i,j] = loss_mse(f_pred(X,w=[[xx[i,j]],[yy[i,j]]]),y)\n",
    "\n",
    "    # 3D visualization\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "    # Left plot\n",
    "    CS = ax1.contour(xx, yy, Z, np.logspace(-2, 3, 20), cmap=plt.cm.jet)\n",
    "    ax1.scatter(w[0],w[1], c='r')\n",
    "    ax1.plot(w_iters[:,0],w_iters[:,1])\n",
    "\n",
    "    # Right plot\n",
    "    ax2.plot_surface(xx, yy, Z, rstride=1, cstride=1, alpha=0.6, cmap=plt.cm.jet)\n",
    "    ax2.set_zlabel('Loss $L(w_0,w_1)$')\n",
    "    ax2.set_zlim(Z.min(),Z.max())\n",
    "    #ax2.view_init(elev=10, azim=-120)\n",
    "\n",
    "    # plot gradient descent\n",
    "    Z2 = np.zeros([max_iter])\n",
    "    for i in range(max_iter):\n",
    "        w0 = w_iters[i,0]\n",
    "        w1 = w_iters[i,1]\n",
    "        Z2[i] = loss_mse(f_pred(X,w=[[w0],[w1]]),y)\n",
    "    ax2.plot(w_iters[:,0],w_iters[:,1],Z2)\n",
    "    ax2.scatter(w[0],w[1],loss_mse(f_pred(X,w=[w[0],w[1]]),y), c='r')\n",
    "\n",
    "    # settings common to both plots\n",
    "    for ax in fig.axes:\n",
    "        ax.set_xlabel(r'$w_0$', fontsize=17)\n",
    "        ax.set_ylabel(r'$w_1$', fontsize=17)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run plot_gradient_descent function\n",
    "w_init = np.array([0.2,-1.4])[:,None]\n",
    "tau = 0.01\n",
    "max_iter = 200\n",
    "plot_gradient_descent(X,y,w_init,tau,max_iter) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run plot_gradient_descent function\n",
    "w_init = np.array([0.2,-1.4])[:,None]\n",
    "tau = 0.01\n",
    "max_iter = 2000\n",
    "plot_gradient_descent(X,y,w_init,tau,max_iter) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
