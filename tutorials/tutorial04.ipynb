{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE9010: Introduction to Data Science\n",
    "## Semester 2 2017/18\n",
    "## Xavier Bresson\n",
    "<hr>\n",
    "\n",
    "## Tutorial 4: Supervised classification\n",
    "## Objectives\n",
    "### $\\bullet$ Code the logistic regression model \n",
    "### $\\bullet$ Compare with gradient descent and scikit-learn implementations\n",
    "### $\\bullet$ Explore results\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# math library\n",
    "import numpy as np\n",
    "\n",
    "# visualization library\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 3d visualization\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# computational time\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset\n",
    "<hr>\n",
    "The data features $x_i=(x_{i(1)},x_{i(2)})$ represent 2 exam grades for each student $i$. <br>\n",
    "The data label/target, $y_i$, indicates if the student was admitted (value is 1) or rejected (value is 0).\n",
    "\n",
    "What is the number $n$ of training data?<br>\n",
    "Hint: You may use numpy function `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data with numpy\n",
    "data = np.loadtxt('data/admission_dataset.txt', delimiter=',')\n",
    "\n",
    "# number of training data\n",
    "n = #YOUR CODE HERE\n",
    "print('Number of training data=',n)\n",
    "\n",
    "# print\n",
    "print(data[:10,:])\n",
    "print(data.shape)\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the dataset distribution\n",
    "<hr>\n",
    "\n",
    "Plot the training data points.<br>\n",
    "Hint: You may use matplotlib function `scatter(x,y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = data[:,0] # exam grade 1\n",
    "x2 = data[:,1] # exam grade 2\n",
    "idx_admit = (data[:,2]==1) # index of students who were admitted\n",
    "idx_rejec = (data[:,2]==0) # index of students who were rejected\n",
    "\n",
    "plt.figure(1,figsize=(6,6))\n",
    "plt.scatter(x1[#YOUR CODE HERE], x2[#YOUR CODE HERE], s=60, c='r', marker='+', linewidths=2, label='Admitted') \n",
    "plt.scatter(x1[#YOUR CODE HERE], x2[#YOUR CODE HERE], s=60, c='b', marker='o', linewidths=2, label='Rejected') \n",
    "plt.title('Training data')\n",
    "plt.xlabel('Exam grade 1')\n",
    "plt.ylabel('Exam grade 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sigmoid/logistic function\n",
    "<hr>\n",
    "$$\n",
    "\\sigma(\\eta) = \\frac{1}{1+e^{-\\eta}}\n",
    "$$\n",
    "\n",
    "Define and plot the sigmoid function for values in [-10,10]:<br>\n",
    "Hint: You may use functions `np.exp`, `np.linspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sigmoid_f =  #YOUR CODE HERE\n",
    "    return sigmoid_f \n",
    "\n",
    "\n",
    "# plot\n",
    "x_values = np.linspace(-10,10)\n",
    "plt.figure(2)\n",
    "plt.plot(x_values,sigmoid(x_values))\n",
    "plt.title(\"Sigmoid function\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the logistic regression/classification predictive function \n",
    "<hr>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_w(x) &= \\sigma(w_0 + w_1 x_{(1)} + w_2 x_{(2)})\\\\\n",
    "&= \\sigma(X w)\n",
    "\\end{aligned}\n",
    "$$\n",
    "with \n",
    "<br>\n",
    "$$\n",
    "X = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "1 & x_{1(1)} & x_{1(2)} \\\\ \n",
    "1 & x_{2(1)} & x_{2(2)} \\\\ \n",
    "\\vdots\\\\\n",
    "1 & x_{n(1)} & x_{n(2)} \n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\textrm{ and }\n",
    "\\quad\n",
    "w = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\ \n",
    "w_2\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\Rightarrow \n",
    "\\quad\n",
    "p_w(x) = \\sigma(X w)  =\n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "\\sigma(w_0 + w_1 x_{1(1)} + w_2 x_{1(2)}) \\\\ \n",
    "\\sigma(w_0 + w_1 x_{2(1)} + w_2 x_{2(2)}) \\\\ \n",
    "\\vdots\\\\\n",
    "\\sigma(w_0 + w_1 x_{n(1)} + w_2 x_{n(2)})\n",
    "\\end{array} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Implement the vectorized version of the logistic regression function. <br>\n",
    "\n",
    "Check your code correctness: The first 3 values of $p_w(x)$ are [2.41e-10,1.44e-07,7.61e-10] for $w_0=-10, w_1=0.1, w_2=-0.2$. <br>\n",
    "\n",
    "Hint: Respect the sizes of $X$ and $w$ when carrying out linear algebra multiplications. Use the new function `sigmoid`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the data matrix X\n",
    "n = data.shape[0]\n",
    "X = np.ones([n,3]) \n",
    "X[:,1:3] = data[:,0:2]\n",
    "print(X.shape)\n",
    "print(X[:5,:])\n",
    "\n",
    "\n",
    "# parameters vector\n",
    "w = np.array([-10,0.1,-0.2])[:,None] # [:,None] adds a singleton dimension\n",
    "print(w.shape)\n",
    "\n",
    "\n",
    "# predictive function definition\n",
    "def f_pred(X,w): \n",
    "    p =  #YOUR CODE HERE\n",
    "    return p\n",
    "\n",
    "\n",
    "# Test predicitive function \n",
    "y_pred = f_pred(X,w)\n",
    "print(y_pred[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the logistic regression loss \n",
    "<hr>\n",
    "$$\n",
    "L(w)=-\\frac{1}{n} \\sum_{i=1}^n \\ \\Big(  y_i \\log(p_w(x_i)) + (1-y_i)\\log(1-p_w(x_i)) \\Big)\n",
    "$$\n",
    "\n",
    "The vectorized representation is:\n",
    "$$\n",
    "L(w)=-\\frac{1}{n} \\Big( y^T \\log(p_w(x)) + (1-y)^T \\log(1-p_w(x)) \\Big)\n",
    "$$\n",
    "with \n",
    "<br>\n",
    "$$\n",
    "p_w(x)= \\sigma(Xw)=\n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "\\sigma(w_0 + w_1 x_{1(1)} + w_2 x_{1(2)}) \\\\ \n",
    "\\sigma(w_0 + w_1 x_{2(1)} + w_2 x_{2(2)}) \\\\ \n",
    "\\vdots\\\\\n",
    "\\sigma(w_0 + w_1 x_{n(1)} + w_2 x_{n(2)})\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\textrm{ and }\n",
    "\\quad\n",
    "y = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Implement the vectorized version of the logistic regression loss function. <br>\n",
    "\n",
    "Check your code correctness: The loss values is $10.39$ for $w_0=-10, w_1=0.1, w_2=-0.2$. <br>\n",
    "\n",
    "Hint: Respect the sizes of $X$, $w$ and $y$ when carrying out linear algebra multiplications. You may use numpy functions `.T`, `np.log`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function definition\n",
    "def loss_logreg(y_pred,y): \n",
    "    n = len(y)\n",
    "    loss =  #YOUR CODE HERE\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Test loss function \n",
    "y = data[:,2][:,None] # label \n",
    "print(y.shape)\n",
    "#print(y)\n",
    "y_pred = f_pred(X,w) # prediction\n",
    "loss = loss_logreg(y_pred,y)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define the gradient of the logistic regression loss \n",
    "<hr>\n",
    "\n",
    "Given the loss\n",
    "$$\n",
    "L(w)=-\\frac{1}{n} \\Big( y^T \\log(p_w(x)) + (1-y)^T \\log(1-p_w(x)) \\Big)\n",
    "$$\n",
    "The gradient is given by  \n",
    "$$\n",
    "\\frac{\\partial}{\\partial w} L(w) = \\frac{2}{n} X^T(p_w(x)-y)\n",
    "$$\n",
    "\n",
    "\n",
    "Implement the vectorized version of the gradient of the logistic regression loss function. <br>\n",
    "\n",
    "Check your code correctness: The gradient value is $[-1.19,-89.66,-88.74]$ for $w_0=-10, w_1=0.1, w_2=-0.2$. <br>\n",
    "\n",
    "Hint: Respect the sizes of $X$, $w$ and $y$ when carrying out linear algebra multiplications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient function definition\n",
    "def grad_loss(y_pred,y,X):\n",
    "    n = len(y)\n",
    "    grad =  #YOUR CODE HERE\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Test grad function \n",
    "y_pred = f_pred(X,w)\n",
    "grad = grad_loss(y_pred,y,X)\n",
    "print(grad)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement the gradient descent algorithm \n",
    "<hr>\n",
    "\n",
    "Vectorized implementation: \n",
    "$$\n",
    "w^{k+1} = w^{k} - \\tau  \\frac{2}{n} X^T(p_w(x)-y)\n",
    "$$\n",
    "\n",
    "**7.1** Implement the vectorized version of the gradient descent function. <br>\n",
    "Check your code correctness: The $w^{k}$ value after $1000$ iterations is [-10.004, 0.0854, 0.0792] for initial values $w_0=-10, w_1=0.1, w_2=-0.2$<br> and the loss value $L$ is 0.272.<br>\n",
    "\n",
    "\n",
    "**7.2** Plot the loss values $L(w^k)$ w.r.t. iteration $k$ the number of iterations.<br>\n",
    "\n",
    "**7.3** Try with initial values $w_0=0, w_1=0, w_2=0$. What is the value of $L$ after $1000$ iterations?<br>\n",
    "\n",
    "Hint: You may use a table to store the values of $L(w^k)$ at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent function definition\n",
    "def grad_desc(X, y , w_init=np.array([0,0,0])[:,None] ,tau=1e-4, max_iter=500):\n",
    "\n",
    "    L_iters = np.zeros([max_iter]) # record the loss values\n",
    "    w_iters = np.zeros([max_iter,2]) # record the loss values\n",
    "    w = w_init # initialization\n",
    "    for i in range(max_iter): # loop over the iterations\n",
    "        y_pred =  # linear predicition function  #YOUR CODE HERE\n",
    "        grad_f =  # gradient of the loss #YOUR CODE HERE\n",
    "        w =  # update rule of gradient descent #YOUR CODE HERE\n",
    "        L_iters[i] = loss_logreg(y_pred,y) # save the current loss value \n",
    "        w_iters[i,:] = w[0],w[1] # save the current w value \n",
    "        \n",
    "    return w, L_iters, w_iters\n",
    "\n",
    "\n",
    "# run gradient descent algorithm\n",
    "start = time.time()\n",
    "w_init = np.array([-10,0.1,-0.2])[:,None]\n",
    "#w_init = np.array([0,0,0])[:,None]\n",
    "tau = 1e-4; max_iter = 1000\n",
    "w, L_iters, w_iters = grad_desc(X,y,w_init,tau,max_iter)\n",
    "print('Time=',time.time() - start)\n",
    "print(L_iters[-1])\n",
    "print(w)\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(3)\n",
    "plt.plot(np.array(range(max_iter)), L_iters)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot the decision boundary\n",
    "<hr>\n",
    "It is defined by all points \n",
    "$$\n",
    "x=(x_{(1)},x_{(2)}) \\quad \\textrm{ such that } \\quad p_w(x) = 0.5\n",
    "$$\n",
    "\n",
    "Hint: You may use numpy and matplotlib functions `np.meshgrid`, `np.linspace`, `reshape`, `contour`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute values p(x) for multiple data points x\n",
    "x1_min, x1_max = X[:,1].min(), X[:,1].max() # min and max of grade 1\n",
    "x2_min, x2_max = X[:,2].min(), X[:,2].max() # min and max of grade 2\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max)) # create meshgrid\n",
    "X2 = np.ones([np.prod(xx1.shape),3]) \n",
    "X2[:,1] = xx1.reshape(-1)\n",
    "X2[:,2] = xx2.reshape(-1)\n",
    "p = f_pred(X2,w)\n",
    "p = p.reshape(xx1.shape)\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(4,figsize=(6,6))\n",
    "plt.scatter(x1[#YOUR CODE HERE], x2[#YOUR CODE HERE], s=60, c='r', marker='+', linewidths=2, label='Admitted') #YOUR CODE HERE\n",
    "plt.scatter(x1[#YOUR CODE HERE], x2[#YOUR CODE HERE], s=60, c='b', marker='o', linewidths=2, label='Rejected') #YOUR CODE HERE\n",
    "plt.contour(xx1, xx2, #YOUR CODE HERE, [#YOUR CODE HERE], linewidths=2, colors='k') #YOUR CODE HERE\n",
    "plt.xlabel('Exam grade 1')\n",
    "plt.ylabel('Exam grade 2')\n",
    "plt.legend()\n",
    "plt.title('Decision boundary')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# record p values\n",
    "p_gd = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison with Scikit-learn logistic regression algorithm\n",
    "<hr>\n",
    "What is the loss value of the Scikit-learn solution? <br>\n",
    "Compare with the loss value given by gradient descent?<br>\n",
    "What do we need to do to get a better loss (and solution) with gradient descent? \n",
    "\n",
    "Hint: You may use scikit-learn function `LogisticRegression(C=1e6)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run logistic regression with scikit-learn\n",
    "start = time.time()\n",
    "logreg_sklearn = LogisticRegression(C=1e6) # scikit-learn logistic regression\n",
    "logreg_sklearn.fit(data[:,0:2], data[:,2]) # learn the model parameters #YOUR CODE HERE\n",
    "print('Time=',time.time() - start)\n",
    "\n",
    "\n",
    "# compute loss value\n",
    "w_sklearn = np.zeros([3,1])\n",
    "w_sklearn[0,0] = logreg_sklearn.intercept_\n",
    "w_sklearn[1:3,0] = logreg_sklearn.coef_\n",
    "print(w_sklearn)\n",
    "loss_sklearn = loss_logreg(f_pred(X,w_sklearn),data[:,2][:,None])\n",
    "print('loss sklearn=',loss_sklearn)\n",
    "print('loss gradient descent=',L_iters[-1]) \n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(4,figsize=(6,6))\n",
    "plt.scatter(x1[#YOUR CODE HERE], x2[#YOUR CODE HERE], s=60, c='r', marker='+', linewidths=2, label='Admitted') #YOUR CODE HERE\n",
    "plt.scatter(x1[#YOUR CODE HERE], x2[#YOUR CODE HERE], s=60, c='b', marker='o', linewidths=2, label='Rejected') #YOUR CODE HERE\n",
    "plt.xlabel('Exam grade 1')\n",
    "plt.ylabel('Exam grade 2')\n",
    "\n",
    "x1_min, x1_max = X[:,1].min(), X[:,1].max() # grade 1\n",
    "x2_min, x2_max = X[:,2].min(), X[:,2].max() # grade 2\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max)) # create meshgrid\n",
    "X2 = np.ones([np.prod(xx1.shape),3]) \n",
    "X2[:,1] = xx1.reshape(-1)\n",
    "X2[:,2] = xx2.reshape(-1)\n",
    "p = f_pred(X2,w_sklearn)\n",
    "p = p.reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, #YOUR CODE HERE, [#YOUR CODE HERE], linewidths=2, colors='m');\n",
    "plt.contour(xx1, xx2, p_gd, [0.5], linewidths=2, colors='k');\n",
    "\n",
    "plt.title('Decision boundary (black with gradient descent and magenta with scikit-learn)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Probability for a student to be admitted with the grades (45,85)?\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probability of admission \n",
    "y_pred =  #YOUR CODE HERE\n",
    "print('Probability of admission is',y_pred[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
