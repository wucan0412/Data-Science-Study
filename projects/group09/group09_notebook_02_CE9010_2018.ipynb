{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data from PropertyGuru \n",
    "The following libraries are needed to run this file\n",
    "<ul> \n",
    "    <li> selenium - for scraping\n",
    "    <li> random - for inducing random factor while scraping in order to avoid being blocked\n",
    "    <li> calendar\n",
    "    <li> math \n",
    "</ul> \n",
    "PS : geckodriver must be in the same directory as this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# remove warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a firefox driver with specified parameter\n",
    "firefox_profile = webdriver.FirefoxProfile()\n",
    "firefox_profile.set_preference(\"general.useragent.override\", \"Mozilla/5.0 (Android 4.4; Mobile; rv:41.0) Gecko/41.0 Firefox/41.0\")\n",
    "\n",
    "firefox_capabilities = DesiredCapabilities.FIREFOX\n",
    "firefox_capabilities['marionette'] = True\n",
    "firefox_capabilities['binary'] = 'tools/firefox/firefox-bin'\n",
    "driver = webdriver.Firefox(firefox_profile=firefox_profile, capabilities=firefox_capabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "mrt_data = pd.read_csv('scraped_data/available_MRT.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start scraping the required information from PropertyGuru\n",
    "\n",
    "for j in range(len(mrt_data)):\n",
    "        mrt = mrt_data.iloc[j].MRT\n",
    "        print(\"Getting %s..\"%(mrt))\n",
    "        \n",
    "        base_url = mrt_data.iloc[j].link\n",
    "        driver.get(base_url)\n",
    "        print(\"Waiting at most 10 seconds\")\n",
    "        time.sleep(random.randint(1,10))\n",
    "        \n",
    "        new_base_url = driver.current_url\n",
    "        \n",
    "        print(new_base_url)\n",
    "        target_num = int(driver.find_element_by_xpath('//h1[@class=\"title search-title\"]//span').text.replace(',',''))\n",
    "\n",
    "        '''\n",
    "        while base_url[4:] == new_base_url[5:]:\n",
    "            new_base_url = driver.current_url\n",
    "            \n",
    "        while True:\n",
    "            try:\n",
    "                target_num = int(driver.find_element_by_xpath('//h1[@class=\"title search-title\"]//span').text)\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        '''    \n",
    "        pages = math.ceil(target_num/20)\n",
    "    \n",
    "        \n",
    "        for p in range(1,pages+1):\n",
    "            url = new_base_url + '/%s'% (p)\n",
    "            print(\"URL\")\n",
    "            print(url)\n",
    "            driver.get(url)\n",
    "            \n",
    "            time.sleep(random.randint(5,20))\n",
    "\n",
    "            chunk_obj=[]\n",
    "            \n",
    "            chunk_obj = driver.find_elements_by_xpath(\"//div[@class='row']\")\n",
    "            chunk = [x.text for x in chunk_obj]\n",
    "\n",
    "            '''\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk_obj = driver.find_elements_by_xpath(\"//div[@class='row']\")\n",
    "                    chunk = [x.text for x in chunk_obj]\n",
    "                    if (len(chunk_obj) == 22):\n",
    "                        break\n",
    "                    if (p == pages and len(chunk_obj) == (target_num%20 + 2)):\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            '''    \n",
    "            \n",
    "            \n",
    "            #Cleaning\n",
    "            chunk = [chunk[i].split('\\n') for i in range(len(chunk))]\n",
    "            \n",
    "            del chunk[0]\n",
    "            del chunk[-1]\n",
    "            \n",
    "            print(len(chunk),' records found.')\n",
    "            \n",
    "            print(\"Tidying record...\")\n",
    "            \n",
    "            chunk = [chunk[i][1:] if len(chunk[i])>7 else chunk[i] for i in range(len(chunk))]\n",
    "            #Remove records with size less than 7\n",
    "            chunk = [chunk[i]  for i in range(len(chunk)) if len(chunk[i])>=7]\n",
    "            \n",
    "            data = pd.DataFrame(columns=['name','details','size','address','available time','bed','bath','price','nearest MRT','distance'])#'owner','posted on'])\n",
    "            \n",
    "            \n",
    "            data['name'] = [chunk[i][0] for i in range(len(chunk))]\n",
    "            data['details'] = [chunk[i][1] for i in range(len(chunk))]\n",
    "            data['address'] = [chunk[i][2] for i in range(len(chunk))]\n",
    "            \n",
    "            temp = [chunk[i][3].split('Â·') for i in range(len(chunk))]\n",
    "            data['size'] = [temp[i][0] for i in range(len(temp))]\n",
    "            data['available time'] = [temp[i][1] if len(temp[i])==2 else '' for i in range(len(temp)) ]\n",
    "            \n",
    "            #Getting number of beds and bath\n",
    "            print(\"Getting number of beds and baths...\")\n",
    "            temp2 = [chunk[i][4].split(' ') for i in range(len(chunk))]\n",
    "            temp_bed = []\n",
    "            temp_bath = []\n",
    "            \n",
    "            for i in range(len(temp2)):\n",
    "                if(len(temp2[i]) == 2):\n",
    "                    temp_bed.append(temp2[i][0])\n",
    "                    temp_bath.append(temp2[i][1])\n",
    "                else:\n",
    "                    try:\n",
    "                        temp_bed.append(temp2[i][0])\n",
    "                    except IndexError as e:\n",
    "                        #print(e)\n",
    "                        print(\"At record number %s. 0 had been appended\"%i)\n",
    "                        temp_bed.append(0)\n",
    "                    try:\n",
    "                        temp_bath.append(temp2[i][1])\n",
    "                    except IndexError as e:\n",
    "                        #print(e)\n",
    "                        print(\"At record number %s. 0 had been appended\"%i)\n",
    "                        temp_bath.append(0)\n",
    "        \n",
    "            data['bed'] = temp_bed\n",
    "            data['bath'] = temp_bath         \n",
    "            data['price'] = [chunk[i][5] for i in range(len(temp))]\n",
    "            data['distance'] = [re.search('(\\d).(\\d+) km',x).group() if re.search('(\\d).(\\d+) km',x) is not None else '' for x in data.address ]\n",
    "            data['address'] = [x.split(' (')[0] for x in data.address]\n",
    "            data['nearest MRT ID'] = [mrt for i in range(len(data))]\n",
    "            \n",
    "            now = datetime.datetime.now()\n",
    "            data.to_csv('scraped_data/rental_data_%s_%s%s%s.csv'%(mrt,now.year,calendar.month_name[now.month],now.day),mode='a',header=False)\n",
    "            print('%s page %s saved'%(mrt,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\*Note that the last cell was not being runned on Jupyter Notebook as the process took too long (around half day). Besides, sometimes there is also some captcha process to be passed and hence this cell need to be edited a bit and run again, resulting in the existence of rental_data_(MRT station Name)_p2/p3... in scrapped_data folder "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
