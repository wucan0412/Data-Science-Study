{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03 : GatedGCNs for Molecule Property Regression - demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    file_name = '01_GatedGCNs_Molecule_Regression.ipynb'\n",
    "    import subprocess\n",
    "    path_to_file = subprocess.check_output('find . -type f -name ' + str(file_name), shell=True).decode(\"utf-8\")\n",
    "    print(path_to_file)\n",
    "    path_to_file = path_to_file.replace(file_name,\"\").replace('\\n',\"\")\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !apt-get install python-rdkit librdkit1 rdkit-data -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import sys\n",
    "import collections\n",
    "import os\n",
    "\n",
    "# remove warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# molecule toolkit\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda not available\n"
     ]
    }
   ],
   "source": [
    "notebook_mode = True\n",
    "\n",
    "# select GPU for notebook\n",
    "if notebook_mode == True:\n",
    "    gpu_id = 0  # select GPU id: 0,1,2,3\n",
    "    server_id = 181\n",
    "    \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    gpu_id = -1\n",
    "    server_id = -1\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "class bucket(object):\n",
    "    \"\"\"\n",
    "    INPUT: list of smiles corresponding to molecules of the SAME size (N atoms),\n",
    "           list of corresponding rdkit molecules \n",
    "           atom and bond dictionaries\n",
    "           \n",
    "    ATTRIBUTE: self.atom: Tensor of size bs x N     (atom composition of the molecule)\n",
    "               self.rep:  Tensor of size bs x N     (repeat feature) \n",
    "               self.bond: Tensor of size bs x N x N (adjacency matrix)\n",
    "               self.smile: list containing all the smiles \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smile_list, rdkitmol_list, atom_dict, bond_dict ):\n",
    "        \n",
    "        self.N= rdkitmol_list[0].GetNumAtoms()\n",
    "        self.bs = len(rdkitmol_list)\n",
    "        \n",
    "        self.atom  =  torch.zeros( self.bs , self.N).long()\n",
    "        self.rep   =  torch.zeros( self.bs , self.N).long()\n",
    "        self.bond  =  torch.zeros( self.bs , self.N, self.N).long()\n",
    "        self.smile =  smile_list\n",
    "        \n",
    "        for idx, mol in enumerate(rdkitmol_list):\n",
    "            \n",
    "            n = mol.GetNumAtoms()\n",
    "            if n != self.N:\n",
    "                print('ERROR: mol does not have right size')\n",
    "                \n",
    "            at,r,bd = rdkitMol2pytorchTensor(mol, atom_dict, bond_dict)\n",
    "            self.atom[idx]=at\n",
    "            self.rep[idx]=r\n",
    "            self.bond[idx]=bd\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.bs\n",
    "    \n",
    "    \n",
    "class bucket_helper(object):\n",
    "    \"\"\"\n",
    "    provide a mapping from bucket idx to size of molecule contained in the given bucket\n",
    "    \"\"\" \n",
    "    def __init__(self, data):\n",
    "        self.bucket2size=[]\n",
    "        self.size2bucket={}\n",
    "        self.num_molecules=[]\n",
    "\n",
    "        for idx,bucket in enumerate(data):\n",
    "            sz=bucket.N\n",
    "            self.bucket2size.append(sz)\n",
    "            self.size2bucket[sz]=idx\n",
    "            \n",
    "            \n",
    "            \n",
    "data_folder= 'datasets/dataQM9/'\n",
    "\n",
    "pickle_in = open(data_folder + \"train.pickle\",\"rb\")\n",
    "data_train = pickle.load(pickle_in)\n",
    "pickle_in = open(data_folder + \"test.pickle\",\"rb\")\n",
    "data_test = pickle.load(pickle_in)\n",
    "\n",
    "import dictionaries as dic\n",
    "\n",
    "pickle_in = open(data_folder + \"atom_dict.pickle\",\"rb\")\n",
    "atom_dict = pickle.load(pickle_in)\n",
    "pickle_in = open(data_folder + \"bond_dict.pickle\",\"rb\")\n",
    "bond_dict = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(data_folder + \"info_train.pickle\",\"rb\")\n",
    "info_train = pickle.load(pickle_in)\n",
    "pickle_in = open(data_folder + \"info_test.pickle\",\"rb\")\n",
    "info_test = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print histogram of molecule sizes\n",
      "\n",
      "############# Train Molecules ##############\n",
      "\n",
      "number of molecule of size 1: \t 2\n",
      "number of molecule of size 2: \t 5\n",
      "number of molecule of size 3: \t 8\n",
      "number of molecule of size 4: \t 25\n",
      "number of molecule of size 5: \t 102\n",
      "number of molecule of size 6: \t 535\n",
      "number of molecule of size 7: \t 2763\n",
      "number of molecule of size 8: \t 15552\n",
      "number of molecule of size 9: \t 94887\n",
      "\n",
      "############# Test Molecules ##############\n",
      "\n",
      "number of molecule of size 4: \t 1\n",
      "number of molecule of size 5: \t 13\n",
      "number of molecule of size 6: \t 36\n",
      "number of molecule of size 7: \t 222\n",
      "number of molecule of size 8: \t 1387\n",
      "number of molecule of size 9: \t 8341\n"
     ]
    }
   ],
   "source": [
    "print('Print histogram of molecule sizes')\n",
    "\n",
    "print('\\n############# Train Molecules ##############\\n')\n",
    "\n",
    "data = data_train\n",
    "for bucket in data:\n",
    "    print('number of molecule of size {}: \\t {}'.format(bucket.N, len(bucket)))\n",
    "    \n",
    "print('\\n############# Test Molecules ##############\\n')\n",
    "      \n",
    "data = data_test\n",
    "for bucket in data:\n",
    "    print('number of molecule of size {}: \\t {}'.format(bucket.N, len(bucket)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one pucket of same molecules size, here molecules with 7 atoms\n",
    "\n",
    "def keep_bucket_of_size(dataset,sz_to_keep):\n",
    "    new_dataset=[]\n",
    "    for bucket in dataset:\n",
    "        if bucket.N in sz_to_keep:\n",
    "            new_dataset.append(bucket)\n",
    "    return new_dataset\n",
    "\n",
    "sz_to_keep = [7] # [8,9] \n",
    "data_test  = keep_bucket_of_size(data_test,sz_to_keep)\n",
    "data_train = keep_bucket_of_size(data_train,sz_to_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC12NC1C1OC12\n",
      "tensor([1, 1, 2, 1, 1, 0, 1])\n",
      "['O', 'C', 'N', 'F']\n",
      "tensor([[0, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0, 0, 0],\n",
      "        [0, 1, 1, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 0, 1, 1, 0]])\n",
      "['NONE', 'SINGLE', 'DOUBLE', 'AROMATIC', 'TRIPLE']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x1245a58a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize dataset and molecule with RDKit\n",
    "\n",
    "idx_bucket = 0\n",
    "idx_mol = 45\n",
    "\n",
    "smile = data_train[idx_bucket].smile[idx_mol]\n",
    "print(smile)\n",
    "\n",
    "print(data_train[idx_bucket].atom[idx_mol])\n",
    "print(atom_dict.idx2word)\n",
    "\n",
    "print(data_train[idx_bucket].bond[idx_mol])\n",
    "print(bond_dict.idx2word)\n",
    "\n",
    "mol = Chem.MolFromSmiles(smile)\n",
    "mol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([2763])\n"
     ]
    }
   ],
   "source": [
    "def compute_bucket_stats(data):\n",
    "\n",
    "    size_of_mol_in_bucket=[]\n",
    "    num_mol_in_bucket=[]\n",
    "\n",
    "    for idx,bucket in enumerate(data):\n",
    "        size_of_mol_in_bucket.append(bucket.N)\n",
    "        num_mol_in_bucket.append(bucket.bs)\n",
    "\n",
    "    size_of_mol_in_bucket = torch.LongTensor(size_of_mol_in_bucket)\n",
    "    num_mol_in_bucket = torch.LongTensor(num_mol_in_bucket)\n",
    "\n",
    "    return size_of_mol_in_bucket, num_mol_in_bucket\n",
    "\n",
    "\n",
    "possible_sizes, num_mol_per_bucket = compute_bucket_stats(data_train)\n",
    "print(possible_sizes)\n",
    "print(num_mol_per_bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample randomly a mini-bacth of N molecules\n",
    "\n",
    "class sampler_class(object):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "        possible_sizes:   LongTensor containing the possible sizes\n",
    "        num_mol_per_bucket: LongTensor containing the number of molecule for each given bucket\n",
    "        bs: batch size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bs, possible_sizes, num_mol_per_bucket):  \n",
    "        \n",
    "        self.possible_sizes = possible_sizes\n",
    "        self.num_mol_per_bucket = num_mol_per_bucket\n",
    "        self.num_buckets = len(possible_sizes)\n",
    "        self.bs=bs\n",
    "        self.not_empty = True\n",
    "\n",
    "        # compute the number of batches held in each bucket (note: this is a float)\n",
    "        self.num_remaining_batch_per_bucket = torch.floor( num_mol_per_bucket.float() / bs )\n",
    "        \n",
    "        # initialize the idx to zero for each bucket\n",
    "        self.mol_idx_in_bucket=torch.zeros( self.num_buckets ).long()\n",
    "        \n",
    "        \n",
    "    def get_bucket_idx_and_mol_idx(self):\n",
    "          \n",
    "        prob = self.num_remaining_batch_per_bucket / self.num_remaining_batch_per_bucket.sum()\n",
    "        \n",
    "        # choose one bucket at random, \n",
    "        # get num of atoms corresponding to this bucket\n",
    "        # and get the idx of the first molecule for the batch to be extracted\n",
    "        bucket_idx = np.random.choice( self.num_buckets  , p=prob.numpy() )\n",
    "        mol_idx = self.mol_idx_in_bucket[bucket_idx].item()\n",
    "        \n",
    "        # update the trackers\n",
    "        self.num_remaining_batch_per_bucket[bucket_idx] -= 1\n",
    "        self.mol_idx_in_bucket[bucket_idx] += self.bs\n",
    "   \n",
    "        if self.num_remaining_batch_per_bucket.sum().long() == 0:\n",
    "            self.not_empty=False\n",
    "            \n",
    "        return bucket_idx , mol_idx\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 5\n",
    "sampler = sampler_class(batch_size, possible_sizes, num_mol_per_bucket)\n",
    "\n",
    "buck_idx, mol_idx = sampler.get_bucket_idx_and_mol_idx()\n",
    "#print(buck_idx)\n",
    "#print(mol_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb_atoms': 4, 'nb_bonds': 5, 'max_atom_count': 7, 'hidden_dim': 50, 'L': 2}\n"
     ]
    }
   ],
   "source": [
    "# network parameters\n",
    "net_parameters = {}\n",
    "net_parameters['nb_atoms'] = len(atom_dict.idx2word)\n",
    "net_parameters['nb_bonds'] = len(bond_dict.idx2word)\n",
    "net_parameters['max_atom_count'] = data_train[-1].N\n",
    "\n",
    "net_parameters['hidden_dim'] = 50 # debug\n",
    "net_parameters['L'] = 2           # debug\n",
    "\n",
    "if notebook_mode == True:\n",
    "    print(net_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoleculeNet_regression(\n",
      "  (molecule_encoder): molecule_encoder(\n",
      "    (atoms_embedding): Embedding(4, 50)\n",
      "    (bonds_embedding): Embedding(5, 50)\n",
      "    (convnet_layers): ModuleList(\n",
      "      (0): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (W): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (W): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (edges_to_vector): edges_to_vector(\n",
      "      (gate): edge_convnet_feat(\n",
      "        (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (W): Linear(in_features=50, out_features=50, bias=True)\n",
      "      )\n",
      "      (A): Linear(in_features=50, out_features=50, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp): mlp(\n",
      "    (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (V): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "nb_param= 39151\n",
      "torch.Size([5])\n",
      "tensor([-4.0801,  0.5805, -1.0869, -1.1108,  0.7360],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class node_convnet_feat(nn.Module):\n",
    "    \"\"\"\n",
    "    convnet features for nodes\n",
    "    x_i = U*x_i +  sum_j gate_ij * (V*x_j)\n",
    "    size of input x : B x V x H\n",
    "    size of output edge_gate : B x V x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(node_convnet_feat, self).__init__()\n",
    "         \n",
    "        self.U  = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V  = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, edge_gate):\n",
    "        \n",
    "        Ux = self.U(x) # B x V x H\n",
    "        Vx = self.V(x) # B x V x H\n",
    "        Vx = Vx.unsqueeze(1) # extend Vx from \"B x V x H\" to \"B x 1 x V x H\"\n",
    "        gateVx = edge_gate* Vx # B x V x V x H\n",
    "        x_new = Ux + torch.sum( gateVx , dim=2) # B x V x H\n",
    "        \n",
    "        return x_new\n",
    "\n",
    "    \n",
    "class edge_convnet_feat(nn.Module):\n",
    "    \"\"\"\n",
    "    convnet features for edges\n",
    "    e_ij = U*e_ij + V*x_i + W*x_j\n",
    "    size of input x : B x V x H\n",
    "    size of output e : # B x V x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(edge_convnet_feat, self).__init__()\n",
    "         \n",
    "        self.U  = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V  = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.W  = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        \n",
    "        Ue = self.U(e)\n",
    "        Vx = self.V(x)\n",
    "        Wx = self.W(x)\n",
    "        Vx = Vx.unsqueeze(2) # extend Vx from \"B x V x H\" to \"B x V x 1 x H\"\n",
    "        Wx = Wx.unsqueeze(1) # extend Wx from \"B x V x H\" to \"B x 1 x V x H\"\n",
    "        e_new = Ue + Vx + Wx\n",
    "        \n",
    "        return e_new\n",
    "\n",
    "    \n",
    "class bn_node(nn.Module):\n",
    "    \"\"\"\n",
    "    batch normalization for nodes\n",
    "    size of input x : B x V x H\n",
    "    size of output x_bn : B x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(bn_node, self).__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(hidden_dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_trans = x.transpose(1,2).contiguous() # input must be of shape: B x H x V\n",
    "        x_trans_bn = self.bn(x_trans)\n",
    "        x_bn = x_trans_bn.transpose(1,2).contiguous() \n",
    "        \n",
    "        return x_bn\n",
    "\n",
    "    \n",
    "class bn_edge(nn.Module):\n",
    "    \"\"\"\n",
    "    batch normalization for edges\n",
    "    size of input e : B x V x V x H\n",
    "    size of output e_bn : B x V x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(bn_edge, self).__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(hidden_dim) \n",
    "\n",
    "    def forward(self, e):\n",
    "        \n",
    "        e_trans = e.transpose(1,3).contiguous() # input must be of shape: B x H x V x V\n",
    "        e_trans_bn = self.bn(e_trans)\n",
    "        e_bn = e_trans_bn.transpose(1,3).contiguous() \n",
    "        \n",
    "        return e_bn\n",
    "\n",
    "    \n",
    "class edges_to_vector(nn.Module):\n",
    "    \"\"\"\n",
    "    vector representation of all edges \n",
    "    z = sum_ij gate_ij * (A*e_ij)\n",
    "    where gate_ij = sigmoid(U*e_ij + V*x_i + W*x_j)\n",
    "    size of input x : B x V x H\n",
    "    size of output e : # B x V x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(edges_to_vector, self).__init__()\n",
    "        \n",
    "        self.gate = edge_convnet_feat(hidden_dim)\n",
    "        self.A = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        \n",
    "        edge_gate = self.gate(x,e)\n",
    "        edge_gate = F.sigmoid(edge_gate)\n",
    "        Ae = self.A(e)\n",
    "        gateAe = edge_gate * Ae\n",
    "        # sum over all edges ij\n",
    "        z = torch.sum(gateAe, dim=1)\n",
    "        z = torch.sum(z, dim=1)\n",
    "        \n",
    "        return z\n",
    "\n",
    "    \n",
    "class mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    3-layer perceptron class\n",
    "    size of input x : B x H\n",
    "    size of output y : B x 1\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, output_dim):        \n",
    "        super(mlp, self).__init__()\n",
    "        \n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V = nn.Linear(hidden_dim, output_dim, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        Ux = self.U(x) # B x H\n",
    "        y = F.relu(Ux) # B x H\n",
    "        y = self.V(y) # B x H\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class basic_convnet_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    basic convnet class\n",
    "    size of input x : B x V x H\n",
    "    size of input e : B x V x V x H\n",
    "    size of output x_new : B x V x H\n",
    "    size of output e_new : B x V x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(basic_convnet_layer, self).__init__()\n",
    "        \n",
    "        self.node_convnet_feat = node_convnet_feat(hidden_dim)\n",
    "        self.edge_convnet_feat = edge_convnet_feat(hidden_dim)\n",
    "        self.bn_node = bn_node(hidden_dim)      \n",
    "        self.bn_edge = bn_edge(hidden_dim)     \n",
    "        \n",
    "    def forward(self, x, e):\n",
    "        \n",
    "        e_in = e\n",
    "        x_in = x\n",
    "        e_tmp = self.edge_convnet_feat(x_in, e_in) # B x V x V x H\n",
    "        edge_gate = F.sigmoid(e_tmp)\n",
    "        x_tmp = self.node_convnet_feat(x_in, edge_gate)\n",
    "        e_tmp = self.bn_edge(e_tmp)\n",
    "        x_tmp = self.bn_node(x_tmp)\n",
    "        e = F.relu(e_tmp)\n",
    "        x = F.relu(x_tmp)\n",
    "        x_new = x_in + x \n",
    "        e_new = e_in + e \n",
    " \n",
    "        return x_new, e_new\n",
    "\n",
    "\n",
    "class molecule_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    molecule encoder class\n",
    "    size of input train_x_node : B x V\n",
    "    size of input train_x_edge : B x V x V\n",
    "    size of output z : B x H\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_atoms, nb_bonds, hidden_dim, L):        \n",
    "        super(molecule_encoder, self).__init__()\n",
    "        \n",
    "        # atoms embedding\n",
    "        self.atoms_embedding = nn.Embedding(nb_atoms, hidden_dim)\n",
    "\n",
    "        # bonds embedding\n",
    "        self.bonds_embedding = nn.Embedding(nb_bonds, hidden_dim) \n",
    "\n",
    "        # list of convnet layers\n",
    "        convnet_layers = [] \n",
    "        for layer in range(L):\n",
    "            convnet_layers.append(basic_convnet_layer(hidden_dim))\n",
    "        self.convnet_layers = nn.ModuleList(convnet_layers)\n",
    "\n",
    "        # edges to vector \n",
    "        self.edges_to_vector = edges_to_vector(hidden_dim)\n",
    "        \n",
    "        # class variables\n",
    "        self.L = L\n",
    "        \n",
    "    def forward(self, train_x_node, train_x_edge):\n",
    "        \n",
    "        x = self.atoms_embedding(train_x_node) # B x V x H\n",
    "        e = self.bonds_embedding(train_x_edge) # B x V x V x H\n",
    "        for layer in range(self.L):\n",
    "            x,e = self.convnet_layers[layer](x,e) # B x V x H,  B x V x V x H\n",
    "        z = self.edges_to_vector(x,e) # B x H\n",
    "       \n",
    "        return z\n",
    "\n",
    "    \n",
    "class MoleculeNet_regression(nn.Module):\n",
    "    \"\"\"\n",
    "    network for molecule regression\n",
    "    \"\"\"\n",
    "    def __init__(self, net_parameters):        \n",
    "        super(MoleculeNet_regression, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        nb_atoms = net_parameters['nb_atoms']\n",
    "        nb_bonds = net_parameters['nb_bonds']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        L = net_parameters['L']\n",
    "        #print(nb_atoms,nb_bonds,hidden_dim,output_dim,L)\n",
    "        \n",
    "        # molecule encoder: from molecule to vector representation (computed with graph convnet)\n",
    "        self.molecule_encoder = molecule_encoder(nb_atoms, nb_bonds, hidden_dim, L)\n",
    "        \n",
    "        # regression part\n",
    "        output_dim = 1\n",
    "        self.mlp = mlp(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, train_x_node, train_x_edge):\n",
    "        \n",
    "        z = self.molecule_encoder(train_x_node, train_x_edge) # B x H\n",
    "        regression_value = self.mlp(z).squeeze() # B\n",
    "    \n",
    "        return regression_value\n",
    "    \n",
    "        \n",
    "    def loss(self, y, y_target):\n",
    "        \n",
    "        #loss = nn.MSELoss()(y,y_target)\n",
    "        loss = nn.L1Loss()(y,y_target)\n",
    "        \n",
    "        return loss\n",
    "       \n",
    "        \n",
    "    def chemical_accuracy(self, y, y_target):\n",
    "        \n",
    "        chemical_accuracy_LUMO = 0.043\n",
    "        MAE = F.l1_loss(y, y_target)\n",
    "        MAE /= chemical_accuracy_LUMO\n",
    "\n",
    "        return MAE \n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.Adam( self.parameters(), lr=lr )\n",
    "        \n",
    "        return update\n",
    "    \n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# simple forward test   \n",
    "if notebook_mode == True:\n",
    "\n",
    "    # instantiate the network\n",
    "    net = MoleculeNet_regression(net_parameters)\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    print(net)\n",
    "\n",
    "\n",
    "    # number of network parameters\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += np.prod(list(param.data.size()))\n",
    "    print('nb_param=',nb_param)\n",
    "\n",
    "\n",
    "    # forward\n",
    "    batch_size = 5\n",
    "    sampler = sampler_class(batch_size, possible_sizes, num_mol_per_bucket)\n",
    "    buck_idx, mol_idx = sampler.get_bucket_idx_and_mol_idx()\n",
    "    train_x_node = Variable( torch.LongTensor(data_train[buck_idx].atom[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "    train_x_node_count = Variable( torch.LongTensor(data_train[buck_idx].rep[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "    train_x_edge = Variable( torch.LongTensor(data_train[buck_idx].bond[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "    train_y = Variable( torch.FloatTensor(data_train[buck_idx].lumo[mol_idx:mol_idx+batch_size]).type(dtypeFloat) , requires_grad=False)\n",
    "\n",
    "    y = net.forward(train_x_node, train_x_edge) # B x 1\n",
    "    print(y.size())\n",
    "    print(y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'max_epochs': 1, 'decay_rate': 1.25, 'batch_size': 50}\n",
      "tensor(1.5384, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# optimization parameters\n",
    "opt_parameters = {}\n",
    "opt_parameters['learning_rate'] = 0.001\n",
    "opt_parameters['max_epochs'] = 1 ########### debug\n",
    "opt_parameters['decay_rate'] = 1.25\n",
    "opt_parameters['batch_size'] = 50\n",
    "\n",
    "\n",
    "# simple backward test  \n",
    "if notebook_mode == True:\n",
    "\n",
    "    print(opt_parameters)\n",
    "\n",
    "    learning_rate = opt_parameters['learning_rate']\n",
    "    max_epochs = opt_parameters['max_epochs']\n",
    "    decay_rate = opt_parameters['decay_rate']\n",
    "\n",
    "\n",
    "    # compute loss \n",
    "    loss = net.loss(y,train_y)\n",
    "    print(loss)\n",
    "\n",
    "    \n",
    "    # define optimizer\n",
    "    lr = learning_rate\n",
    "    optimizer = net.update(lr) \n",
    "\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7496257554401051 17.433156551014292\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_one_epoch(net,optimizer,opt_parameters):\n",
    "    \"\"\"\n",
    "    train one epoch\n",
    "    \"\"\"\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    batch_size = opt_parameters['batch_size']\n",
    "    \n",
    "    possible_sizes, num_mol_per_bucket = compute_bucket_stats(data_train)\n",
    "    sampler = sampler_class(batch_size, possible_sizes, num_mol_per_bucket)\n",
    "    \n",
    "    shuffling_map = []\n",
    "    for bucket_size in list(num_mol_per_bucket):\n",
    "        shuffling_map.append(np.random.permutation(bucket_size.item()))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    running_nb_data = 0\n",
    "    running_nb_batch = 0\n",
    "    \n",
    "    while sampler.not_empty:\n",
    "        \n",
    "        # extract batch\n",
    "        buck_idx, mol_idx = sampler.get_bucket_idx_and_mol_idx()\n",
    "        x_node = Variable( torch.LongTensor(data_train[buck_idx].atom[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "        x_edge = Variable( torch.LongTensor(data_train[buck_idx].bond[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "        y_target = Variable( torch.FloatTensor(data_train[buck_idx].lumo[mol_idx:mol_idx+batch_size]).type(dtypeFloat) , requires_grad=False)\n",
    "\n",
    "        # forward, backward, optimize\n",
    "        optimizer.zero_grad()\n",
    "        y = net.forward(x_node, x_edge) # B \n",
    "        loss = net.loss(y,y_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute loss, accuracy\n",
    "        running_nb_data += batch_size\n",
    "        running_loss += batch_size* loss.data.item()    \n",
    "        running_acc += batch_size* net.chemical_accuracy(y,y_target).item()   \n",
    "        running_nb_batch += 1 # for intermediate result\n",
    "\n",
    "        # print intermediate result\n",
    "        if 2==1:\n",
    "            if running_nb_batch%100==0:\n",
    "                 print('{}: loss={}'.format(running_nb_data,running_loss/running_nb_data))\n",
    "               \n",
    "            \n",
    "    # loss and acc values for one epoch\n",
    "    loss = running_loss/ running_nb_data\n",
    "    acc = running_acc/ running_nb_data\n",
    "    \n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "    \n",
    "# simple train loop   \n",
    "if notebook_mode==True and gpu_id==-1:\n",
    "    train_loss, train_acc = train_one_epoch(net, optimizer, opt_parameters)\n",
    "    print(train_loss, train_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6023355722427368 14.007803678512573\n"
     ]
    }
   ],
   "source": [
    " \n",
    "def evaluate_test_set(net, opt_parameters):\n",
    "    \"\"\"\n",
    "    evaluate test set\n",
    "    \"\"\"\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    batch_size = opt_parameters['batch_size'] \n",
    "    \n",
    "    possible_sizes, num_mol_per_bucket = compute_bucket_stats(data_test)\n",
    "    sampler = sampler_class(batch_size, possible_sizes, num_mol_per_bucket)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    running_nb_data = 0\n",
    "    \n",
    "    while sampler.not_empty:\n",
    "        \n",
    "        # extract batch\n",
    "        buck_idx, mol_idx = sampler.get_bucket_idx_and_mol_idx()\n",
    "        x_node = Variable( torch.LongTensor(data_test[buck_idx].atom[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "        x_edge = Variable( torch.LongTensor(data_test[buck_idx].bond[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "        y_target = Variable( torch.FloatTensor(data_test[buck_idx].lumo[mol_idx:mol_idx+batch_size]).type(dtypeFloat) , requires_grad=False)\n",
    "    \n",
    "        # forward, backward, optimize\n",
    "        y = net.forward(x_node, x_edge) # B \n",
    "        loss = net.loss(y,y_target)\n",
    "        \n",
    "        # compute loss, accuracy\n",
    "        running_nb_data += batch_size\n",
    "        running_loss += batch_size* loss.data.item()    \n",
    "        running_acc += batch_size* net.chemical_accuracy(y,y_target).item()  \n",
    "\n",
    "        \n",
    "    # loss and acc values for one epoch\n",
    "    loss = running_loss/ running_nb_data\n",
    "    acc = running_acc/ running_nb_data\n",
    "    \n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "    \n",
    "# simple test loop   \n",
    "if notebook_mode==True and gpu_id==-1:\n",
    "    test_loss, test_acc = evaluate_test_set(net, opt_parameters)\n",
    "    print(test_loss, test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############\n",
    "# parameters values in notebook mode\n",
    "###############\n",
    "if notebook_mode == True:\n",
    "\n",
    "    # network parameters\n",
    "    net_parameters = {}\n",
    "    net_parameters['nb_atoms'] = len(atom_dict.idx2word)\n",
    "    net_parameters['nb_bonds'] = len(bond_dict.idx2word)\n",
    "    net_parameters['max_atom_count'] = data_train[-1].N\n",
    "    net_parameters['hidden_dim'] = 50 # 200\n",
    "    net_parameters['L'] = 3 # 12\n",
    "    \n",
    "    # optimization parameters\n",
    "    opt_parameters = {}\n",
    "    opt_parameters['learning_rate'] = 0.001\n",
    "    opt_parameters['max_epochs'] = 20  \n",
    "    opt_parameters['decay_rate'] = 1.2\n",
    "    opt_parameters['batch_size'] = 50\n",
    "    \n",
    "    # save results\n",
    "    args = []\n",
    "    args.append(['max_nb_atoms',net_parameters['max_atom_count']])\n",
    "    args.append(['max_epochs',opt_parameters['max_epochs']])\n",
    "    args.append(['batch_size',opt_parameters['batch_size']])\n",
    "    args.append(['decay_rate',opt_parameters['decay_rate']])\n",
    "    args.append(['learning_rate',opt_parameters['learning_rate']])\n",
    "    args.append(['hidden_dim',net_parameters['hidden_dim']])\n",
    "    args.append(['L',net_parameters['L']])\n",
    "    args.append(['gpu_id',gpu_id])\n",
    "    args.append(['server_id',server_id])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoleculeNet_regression(\n",
      "  (molecule_encoder): molecule_encoder(\n",
      "    (atoms_embedding): Embedding(4, 50)\n",
      "    (bonds_embedding): Embedding(5, 50)\n",
      "    (convnet_layers): ModuleList(\n",
      "      (0): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (W): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (W): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (W): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (edges_to_vector): edges_to_vector(\n",
      "      (gate): edge_convnet_feat(\n",
      "        (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (W): Linear(in_features=50, out_features=50, bias=True)\n",
      "      )\n",
      "      (A): Linear(in_features=50, out_features=50, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp): mlp(\n",
      "    (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (V): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "net_parameters: {'nb_atoms': 4, 'nb_bonds': 5, 'max_atom_count': 7, 'hidden_dim': 50, 'L': 3}\n",
      "opt_parameters: {'learning_rate': 0.001, 'max_epochs': 20, 'decay_rate': 1.2, 'batch_size': 50}\n",
      "nb net parameters: 52101\n",
      "gpu:-1  epoch:0 epoch time:2s total time:0.0h lr:1.00e-03 loss:0.729/0.440 MAE:16.948/10.224 \n",
      "gpu:-1  epoch:1 epoch time:2s total time:0.0h lr:1.00e-03 loss:0.481/0.452 MAE:11.192/10.520 \n",
      "gpu:-1  epoch:2 epoch time:2s total time:0.0h lr:1.00e-03 loss:0.419/0.383 MAE:9.741/8.910 \n",
      "gpu:-1  epoch:3 epoch time:2s total time:0.0h lr:1.00e-03 loss:0.362/0.375 MAE:8.425/8.729 \n",
      "gpu:-1  epoch:4 epoch time:2s total time:0.0h lr:1.00e-03 loss:0.351/0.362 MAE:8.155/8.408 \n",
      "gpu:-1  epoch:5 epoch time:2s total time:0.0h lr:1.00e-03 loss:0.344/0.339 MAE:7.999/7.872 \n",
      "gpu:-1  epoch:6 epoch time:2s total time:0.0h lr:1.00e-03 loss:0.330/0.427 MAE:7.665/9.924 \n",
      "gpu:-1  epoch:7 epoch time:2s total time:0.0h lr:8.33e-04 loss:0.332/0.408 MAE:7.730/9.482 \n",
      "gpu:-1  epoch:8 epoch time:2s total time:0.0h lr:8.33e-04 loss:0.304/0.329 MAE:7.080/7.658 \n",
      "gpu:-1  epoch:9 epoch time:2s total time:0.0h lr:8.33e-04 loss:0.291/0.273 MAE:6.775/6.358 \n",
      "gpu:-1  epoch:10 epoch time:2s total time:0.0h lr:8.33e-04 loss:0.275/0.283 MAE:6.392/6.574 \n",
      "gpu:-1  epoch:11 epoch time:2s total time:0.0h lr:8.33e-04 loss:0.266/0.264 MAE:6.195/6.141 \n",
      "gpu:-1  epoch:12 epoch time:2s total time:0.0h lr:6.94e-04 loss:0.274/0.363 MAE:6.375/8.438 \n",
      "gpu:-1  epoch:13 epoch time:2s total time:0.0h lr:6.94e-04 loss:0.266/0.315 MAE:6.198/7.335 \n",
      "gpu:-1  epoch:14 epoch time:2s total time:0.0h lr:6.94e-04 loss:0.257/0.320 MAE:5.980/7.436 \n",
      "gpu:-1  epoch:15 epoch time:2s total time:0.0h lr:6.94e-04 loss:0.252/0.339 MAE:5.856/7.883 \n",
      "gpu:-1  epoch:16 epoch time:2s total time:0.0h lr:5.79e-04 loss:0.251/0.287 MAE:5.835/6.683 \n",
      "gpu:-1  epoch:17 epoch time:2s total time:0.0h lr:5.79e-04 loss:0.246/0.264 MAE:5.723/6.132 \n",
      "gpu:-1  epoch:18 epoch time:2s total time:0.0h lr:5.79e-04 loss:0.236/0.243 MAE:5.488/5.658 \n",
      "gpu:-1  epoch:19 epoch time:2s total time:0.0h lr:5.79e-04 loss:0.230/0.240 MAE:5.358/5.577 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(args):\n",
    "    \"\"\"\n",
    "    main function\n",
    "    \"\"\"\n",
    "    \n",
    "    # instantiate the network\n",
    "    net = MoleculeNet_regression(net_parameters)\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    print(net)\n",
    "    \n",
    "    # number of network parameters\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += np.prod(list(param.data.size()))\n",
    "    print('net_parameters:',net_parameters)\n",
    "    print('opt_parameters:',opt_parameters)\n",
    "    print('nb net parameters:',nb_param)\n",
    "    \n",
    "    # train parameters\n",
    "    train_loss_old = 1e6\n",
    "    lr = opt_parameters['learning_rate']\n",
    "    decay_rate = opt_parameters['decay_rate']\n",
    "    optimizer = net.update(lr) \n",
    "    max_epochs = opt_parameters['max_epochs']\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = net.update(lr) \n",
    "\n",
    "    # loop over epochs\n",
    "    start = time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        start_epoch = time.time()\n",
    "        \n",
    "        # train, test\n",
    "        train_loss, train_acc = train_one_epoch(net, optimizer, opt_parameters)\n",
    "        test_loss, test_acc = evaluate_test_set(net, opt_parameters)\n",
    "        \n",
    "        # update learning rate \n",
    "        if train_loss > 0.99* train_loss_old:\n",
    "            lr /= decay_rate\n",
    "        optimizer = net.update_learning_rate(optimizer, lr)\n",
    "        train_loss_old = train_loss\n",
    "        \n",
    "        # print intermediate results\n",
    "        print_one_epoch_result = ( 'gpu:{ID}  epoch:{EP} epoch time:{epoch_time:.0f}s total time:{fromstart:.1f}h '\n",
    "                   'lr:{LR:.2e} '\n",
    "                   'loss:{train_loss:.3f}/{test_loss:.3f} '\n",
    "                   'MAE:{train_acc:.3f}/{test_acc:.3f} '.format(\n",
    "                    ID=gpu_id,\n",
    "                    EP=epoch,\n",
    "                    fromstart=(time.time()-start)/3600,          \n",
    "                    epoch_time=time.time()-start_epoch,\n",
    "                    LR= lr,  \n",
    "                    train_loss=train_loss, test_loss=test_loss,\n",
    "                    train_acc=train_acc, test_acc=test_acc ) )\n",
    "        print(print_one_epoch_result)\n",
    "        \n",
    "        \n",
    "          \n",
    "            \n",
    "# run main function           \n",
    "main(args)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
